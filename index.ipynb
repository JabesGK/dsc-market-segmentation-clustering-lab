{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Segmentation with Clustering - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use your knowledge of clustering to perform market segmentation on a real-world dataset!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Use clustering to create and interpret market segmentation on real-world data \n",
    "\n",
    "## Getting Started\n",
    "\n",
    "In this lab, you're going to work with the [Wholesale customers dataset](https://archive.ics.uci.edu/ml/datasets/wholesale+customers) from the UCI Machine Learning datasets repository. This dataset contains data on wholesale purchasing information from real businesses. These businesses range from small cafes and hotels to grocery stores and other retailers. \n",
    "\n",
    "Here's the data dictionary for this dataset:\n",
    "\n",
    "|      Column      |                                               Description                                              |\n",
    "|:----------------:|:------------------------------------------------------------------------------------------------------:|\n",
    "|       FRESH      |                    Annual spending on fresh products, such as fruits and vegetables                    |\n",
    "|       MILK       |                               Annual spending on milk and dairy products                               |\n",
    "|      GROCERY     |                                   Annual spending on grocery products                                  |\n",
    "|      FROZEN      |                                   Annual spending on frozen products                                   |\n",
    "| DETERGENTS_PAPER |                  Annual spending on detergents, cleaning supplies, and paper products                  |\n",
    "|   DELICATESSEN   |                           Annual spending on meats and delicatessen products                           |\n",
    "|      CHANNEL     | Type of customer.  1=Hotel/Restaurant/Cafe, 2=Retailer. (This is what we'll use clustering to predict) |\n",
    "|      REGION      |            Region of Portugal that the customer is located in. (This column will be dropped)           |\n",
    "\n",
    "\n",
    "\n",
    "One benefit of working with this dataset for practice with segmentation is that we actually have the ground-truth labels of what market segment each customer actually belongs to. For this reason, we'll borrow some methodology from supervised learning and store these labels separately, so that we can use them afterward to check how well our clustering segmentation actually performed. \n",
    "\n",
    "Let's get started by importing everything we'll need.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `pandas`, `numpy`, and `matplotlib.pyplot`, and set the standard alias for each. \n",
    "* Use `numpy` to set a random seed of `0`.\n",
    "* Set all matplotlib visualizations to appear inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)  # For reproducibility\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our data and inspect it. You'll find the data stored in `'wholesale_customers_data.csv'`. \n",
    "\n",
    "In the cell below, load the data into a DataFrame and then display the first five rows to ensure everything loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('wholesale_customers_data.csv')\n",
    "# Display the first few rows of the dataset \n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go ahead and store the `'Channel'` column in a separate variable and then drop both the `'Channel'` and `'Region'` columns. Then, display the first five rows of the new DataFrame to ensure everything worked correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = raw_df['Channel'].unique()\n",
    "df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get right down to it and begin our clustering analysis. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Import `KMeans` from `sklearn.cluster`, and then create an instance of it. Set the number of clusters to `2`\n",
    "* Fit it to the data (`df`) \n",
    "* Get the predictions from the clustering algorithm and store them in `cluster_preds` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KMeans from sklearn\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=2, random_state=42)\n",
    "# Fit the model to the data\n",
    "k_means.fit(df)\n",
    "# Predict the clusters\n",
    "\n",
    "cluster_preds = k_means.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use some of the metrics to check the performance. You'll use `calinski_harabasz_score()` and `adjusted_rand_score()`, which can both be found inside [`sklearn.metrics`](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). \n",
    "\n",
    "In the cell below, import these scoring functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start with CH score to get the variance ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz Score: 171.685\n"
     ]
    }
   ],
   "source": [
    "#calculate the Calinski-Harabasz score\n",
    "ch_score = calinski_harabasz_score(df, cluster_preds)\n",
    "#Display the Calinski-Harabasz score\n",
    "print(f'Calinski-Harabasz Score: {ch_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you don't have any other numbers to compare this to, this is a pretty low score, suggesting that the clusters aren't great. \n",
    "\n",
    "Since you actually have ground-truth labels, in this case you can use `adjusted_rand_score()` to check how well the clustering performed. Adjusted Rand score is meant to compare two clusterings, which the score can interpret our labels as. This will tell us how similar the predicted clusters are to the actual channels. \n",
    "\n",
    "Adjusted Rand score is bounded between -1 and 1. A score close to 1 shows that the clusters are almost identical. A score close to 0 means that predictions are essentially random, while a score close to -1 means that the predictions are pathologically bad, since they are worse than random chance. \n",
    "\n",
    "In the cell below, call `adjusted_rand_score()` and pass in `channels` and `cluster_preds` to see how well your first iteration of clustering performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index: -0.031\n"
     ]
    }
   ],
   "source": [
    "#Calculate the Adjusted Rand Index\n",
    "ari_score = adjusted_rand_score(raw_df['Channel'], cluster_preds)\n",
    "#Display the Adjusted Rand Index\n",
    "print(f'Adjusted Rand Index: {ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to these results, the clusterings were essentially no better than random chance. Let's see if you can improve this. \n",
    "\n",
    "### Scaling our dataset\n",
    "\n",
    "Recall that k-means clustering is heavily affected by scaling. Since the clustering algorithm is distance-based, this makes sense. Let's use `StandardScaler` to scale our dataset and then try our clustering again and see if the results are different. \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import and instantiate [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and use it to transform the dataset  \n",
    "* Instantiate and fit k-means to this scaled data, and then use it to predict clusters \n",
    "* Calculate the adjusted Rand score for these new predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_k_means = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "scaled_preds = scaled_k_means.fit_predict(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Calinski-Harabasz Score: 154.899\n",
      "Scaled Adjusted Rand Index: 0.927\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the clustering performance on the scaled data\n",
    "scaled_ch_score = calinski_harabasz_score(scaled_df, scaled_preds)\n",
    "scaled_ari_score = adjusted_rand_score(raw_df['Channel'], scaled_preds)\n",
    "#Display the scores for the scaled data\n",
    "print(f'Scaled Calinski-Harabasz Score: {scaled_ch_score:.3f}')\n",
    "print(f'Scaled Adjusted Rand Index: {scaled_ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big improvement! Although it's not perfect, we can see that scaling our data had a significant effect on the quality of our clusters. \n",
    "\n",
    "## Incorporating PCA\n",
    "\n",
    "Since clustering algorithms are distance-based, this means that dimensionality has a definite effect on their performance. The greater the dimensionality of the dataset, the greater the total area that we have to worry about our clusters existing in. Let's try using Principal Component Analysis to transform our data and see if this affects the performance of our clustering algorithm. \n",
    "\n",
    "Since you've already seen PCA in a previous section, we will let you figure this out by yourself. \n",
    "\n",
    "In the cells below:\n",
    "\n",
    "* Import [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from the appropriate module in sklearn \n",
    "* Create a `PCA` instance and use it to transform our scaled data  \n",
    "* Investigate the explained variance ratio for each Principal Component. Consider dropping certain components to reduce dimensionality if you feel it is worth the loss of information \n",
    "* Create a new `KMeans` object, fit it to our PCA-transformed data, and check the adjusted Rand score of the predictions it makes. \n",
    "\n",
    "**_NOTE:_** Your overall goal here is to get the highest possible adjusted Rand score. Don't be afraid to change parameters and rerun things to see how it changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "# Fit and transform the scaled data\n",
    "pca_df = pca.fit_transform(scaled_df)\n",
    "# Create a DataFrame from the PCA results\n",
    "pca_df = pd.DataFrame(data=pca_df, columns=['PC1', 'PC2'])\n",
    "# Add the cluster predictions to the PCA DataFrame\n",
    "pca_df['Cluster'] = scaled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.38750123 0.22374588]\n",
      "Total Explained Variance: 0.611\n"
     ]
    }
   ],
   "source": [
    "#check the explained variance ratio\n",
    "print(f'Explained Variance Ratio: {pca.explained_variance_ratio_}')\n",
    "print(f'Total Explained Variance: {np.sum(pca.explained_variance_ratio_):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the KMeans model on the PCA data\n",
    "pca_k_means = KMeans(n_clusters=2, random_state=42)\n",
    "# Predict the clusters using PCA data\n",
    "pca_preds = pca_k_means.fit_predict(pca_df[['PC1', 'PC2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Calinski-Harabasz Score: 298.730\n",
      "PCA Adjusted Rand Index: 0.822\n"
     ]
    }
   ],
   "source": [
    "#evaluate the clustering performance on the PCA data\n",
    "pca_ch_score = calinski_harabasz_score(pca_df[['PC1', 'PC2']], pca_preds)\n",
    "pca_ari_score = adjusted_rand_score(raw_df['Channel'], pca_preds)\n",
    "#Display the scores for the PCA data\n",
    "print(f'PCA Calinski-Harabasz Score: {pca_ch_score:.3f}')\n",
    "print(f'PCA Adjusted Rand Index: {pca_ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Question_**:  What was the Highest Adjusted Rand Score you achieved? Interpret this score and determine the overall quality of the clustering. Did PCA affect the performance overall?  How many principal components resulted in the best overall clustering performance? Why do you think this is?\n",
    "\n",
    "Write your answer below this line:\n",
    "\n",
    "__\n",
    "*The best clustering performance was achieved after scaling the data, with an Adjusted Rand Score of 0.927, indicating excellent alignment with the true labels.*\n",
    "\n",
    "*Raw data: ARI = -0.031 (worse than random)*\n",
    "\n",
    "*Scaled data: ARI = 0.927 (strong clustering)*\n",
    "\n",
    "*PCA-transformed data: ARI = 0.822 (good, but slightly lower)*\n",
    "\n",
    "*Scaling had the greatest positive impact. PCA helped but slightly reduced performance, likely due to loss of some informative features.*\n",
    "_____________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional (Level up) \n",
    "\n",
    "### Hierarchical Agglomerative Clustering\n",
    "\n",
    "Now that we've tried doing market segmentation with k-means clustering, let's end this lab by trying with HAC!\n",
    "\n",
    "In the cells below, use [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to make cluster predictions on the datasets we've created and see how HAC's performance compares to k-mean's performance. \n",
    "\n",
    "**_NOTE_**: Don't just try HAC on the PCA-transformed dataset -- also compare algorithm performance on the scaled and unscaled datasets, as well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAC Raw Calinski-Harabasz Score: 147.456\n",
      "HAC Raw Adjusted Rand Index: -0.019\n"
     ]
    }
   ],
   "source": [
    "#HAC on raw data\n",
    "hac_raw = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the model to the raw data\n",
    "hac_raw_preds = hac_raw.fit_predict(df)\n",
    "# Evaluate the clustering performance on the raw data\n",
    "hac_raw_ch_score = calinski_harabasz_score(df, hac_raw_preds)\n",
    "hac_raw_ari_score = adjusted_rand_score(raw_df['Channel'], hac_raw_preds)\n",
    "# Display the scores for the raw data\n",
    "print(f'HAC Raw Calinski-Harabasz Score: {hac_raw_ch_score:.3f}')\n",
    "print(f'HAC Raw Adjusted Rand Index: {hac_raw_ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAC Scaled Calinski-Harabasz Score: 149.648\n",
      "HAC Scaled Adjusted Rand Index: 0.982\n"
     ]
    }
   ],
   "source": [
    "#HAC on scaled data\n",
    "hac_scaled = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the model to the scaled data\n",
    "hac_scaled_preds = hac_scaled.fit_predict(scaled_df)\n",
    "# Evaluate the clustering performance on the scaled data\n",
    "hac_scaled_ch_score = calinski_harabasz_score(scaled_df, hac_scaled_preds)\n",
    "hac_scaled_ari_score = adjusted_rand_score(raw_df['Channel'], hac_scaled_preds)\n",
    "# Display the scores for the scaled data\n",
    "print(f'HAC Scaled Calinski-Harabasz Score: {hac_scaled_ch_score:.3f}')\n",
    "print(f'HAC Scaled Adjusted Rand Index: {hac_scaled_ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAC PCA Calinski-Harabasz Score: 280.822\n",
      "HAC PCA Adjusted Rand Index: 0.858\n"
     ]
    }
   ],
   "source": [
    "#HAC on PCA data\n",
    "hac_pca = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the model to the PCA data\n",
    "hac_pca_preds = hac_pca.fit_predict(pca_df[['PC1', 'PC2']])\n",
    "# Evaluate the clustering performance on the PCA data\n",
    "hac_pca_ch_score = calinski_harabasz_score(pca_df[['PC1', 'PC2']], hac_pca_preds)\n",
    "hac_pca_ari_score = adjusted_rand_score(raw_df['Channel'], hac_pca_preds)\n",
    "# Display the scores for the PCA data\n",
    "print(f'HAC PCA Calinski-Harabasz Score: {hac_pca_ch_score:.3f}')\n",
    "print(f'HAC PCA Adjusted Rand Index: {hac_pca_ari_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Raw Data: Performed poorly (negative ARI), confirming that scaling is essential for distance-based clustering algorithms like HAC.*\n",
    "\n",
    "*Scaled Data: Delivered the highest ARI (0.982), showing near-perfect alignment with true labels. Scaling clearly optimized HAC performance.*\n",
    "\n",
    "*PCA Data: Had the highest CH score, suggesting well-separated clusters, but the ARI (0.858) slightly dropped â€” indicating that some label-relevant information was lost in dimensionality reduction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you used your knowledge of clustering to perform a market segmentation on a real-world dataset. You started with a cluster analysis with poor performance, and then implemented some changes to iteratively improve the performance of the clustering analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
